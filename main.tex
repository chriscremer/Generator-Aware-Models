\pdfoutput=1
\documentclass{article}
\usepackage[utf8]{inputenc}

\usepackage{amssymb,amsmath,amsthm}
\usepackage{breakcites}

\usepackage{tikz}
\usetikzlibrary{bayesnet}

\usepackage{subfig}

\usepackage{multirow}

\usepackage{mathtools}

\title{Generator-Aware Bayesian Variational Autoencoders}
\author{}
\date{July 2017}

\begin{document}

\maketitle

\begin{abstract}
We introduce the idea of generator-aware recognition networks. Inference algorithms that belong to this family include the importance-weighted autoencoder and Hamiltonian variational inference. We formerly define these types of inference networks and explore their benefits.
We also introduce a strong inference algorithm (Hamiltonian Flows). 

% In some scenarios, it is important to have uncertainty in the generator. However, naively using a BNN as the generator will be limited by (mutual info argument). Here we show how to build a GABVAE. 
\end{abstract}



\section{Introduction}

VAEs \cite{vae} overfit in both the recognition and generator networks. The recognition overfitting is solvable. The generator overfitting gets worse with higher latent dimensionality $Q$ and input dimensionality $D$. 

A BVAE is a VAE where we are being Bayesian with the parameters $\theta$ of the generator network. 

Meaning of z is dependent on $\theta$. Therefore the recognition networks needs to be generator-aware. Here we introduce the Generator-Aware Bayesian Variational Autoencoders (GABVAE).


\section{Generator-Aware Bayesian VAE}


\begin{figure}[h]
  \centering
      \subfloat[Generative model]
        {
            \tikz
            {
                \node[latent] (weights) {$\theta$} ;
                \node[latent, left=of weights] (Z) {$Z$} ;
                \node[obs, below=of Z] (X) {$X$} ;
                \edge {weights} {X} ;
                \edge {Z} {X} ;
                
                \plate[inner sep=0.25cm, xshift=0.0cm, yshift=0.12cm] {plate1} {(Z) (X)} {N};
            }  \label{fig:f1}
        }
      \qquad \qquad %\hfill
      \subfloat[VAE Inference model]
        {
            \tikz
            {
                \node[latent] (weights) {$\theta$} ;
                \node[latent, left=of weights] (Z) {$Z$} ;
                \node[obs, below=of Z] (X) {$X$} ;
                % \edge {weights} {Z} ;
                \edge {X} {Z} ;
                
                \plate[inner sep=0.25cm, xshift=0.0cm, yshift=0.12cm] {plate1} {(Z) (X)} {N};
            }  \label{fig:f2}
        }
      \qquad \qquad %\hfill
      \subfloat[BVAE Inference model]
        {
            \tikz
            {
                \node[latent] (weights) {$\theta$} ;
                \node[latent, left=of weights] (Z) {$Z$} ;
                \node[obs, below=of Z] (X) {$X$} ;
                \edge {weights} {Z} ;
                \edge {X} {Z} ;
                
                \plate[inner sep=0.25cm, xshift=0.0cm, yshift=0.12cm] {plate1} {(Z) (X)} {N};
            }  \label{fig:f3}
        }
      \caption{Probabilistic graphical models}
      \label{auxvar}
\end{figure}





 
\subsection{How to summarize the generator?}

\begin{itemize}
  \item Pass the whole network (hypo-network)
  \item Pass the latent variable (MNF)
  \item Other summary statistics
\end{itemize}

Another option is to use HVI \cite{hvi}. 







\subsection{Mutual Info}
Let $\mathbf{x_i} \in \mathbf{X}$ denote the $i$-th data point of the training data, 
$\mathbf{z_i}\in \mathbf{Z}$ its corresponding latent representation and $\mathbf{\theta}$ the parameter variables
of the generator network. Assuming that the true posterior factorises as follows,
\begin{align}
p(\mathbf{z_1}, \mathbf{z_2}, \dots, \mathbf{z_n}, \mathbf{\theta} | \mathbf{x_1}, \mathbf{x_2}, \dots, \mathbf{x_n}) 
&= p(\mathbf{\theta} | \mathbf{X}) p(\mathbf{Z} |\mathbf{X}, \mathbf{\theta})\\
&= p(\mathbf{\theta} | \mathbf{X}) \prod_{i=1}^N p(\mathbf{z_i}|\mathbf{x_i}, \mathbf{\theta})
\end{align}
Then, some optimal approximate posterior with the same factorial structure, 
$q^*(\mathbf{Z}, \mathbf{\theta}|\mathbf{X}, \phi_{ga}) = q^*(\mathbf{\theta} | \mathbf{X}, \phi_{ga}) 
\prod_{i=1}^N q^*(\mathbf{z_i}|\mathbf{x_i}, \mathbf{\theta}, \phi_{ga})$, parameterised by $\phi_{ga}$
(``ga'' stands for generator-aware) from a rich enough variational family would be able to match the true posterior and set the 
KL-divergence to zero. More formally, 
\begin{align}
D_{KL}\Bigl(q^*(\mathbf{\theta} | \mathbf{X}, \phi_{ga}) 
\prod_{i=1}^N q^*(\mathbf{z_i}|\mathbf{x_i}, \mathbf{\theta}, \phi_{ga}) || p(\mathbf{Z}, \theta| \mathbf{X})\Bigr) = 0
\end{align}
However, if our approximate posterior does not take into account $\theta$ when predicting $\mathbf{z}$
given $\mathbf{x}$, it is possible that the KL-divergence remain positive even under the optimal approximation, 
however rich the variational family may be.
\begin{align}
D_{KL}\Bigl(q^*(\mathbf{\theta} | \mathbf{X}, \phi_{ga}) 
\prod_{i=1}^N q^*(\mathbf{z_i}|\mathbf{x_i}, \phi_{ga}) || p(\mathbf{Z}, \theta| \mathbf{X})\Bigr) > 0
\end{align}
The loss of information could be measured by the KL-divergence between these two approximations. 
\begin{align}
&D_{KL}\Bigl(q^*(\mathbf{Z}, \theta |\mathbf{X}) || 
q^*(\mathbf{Z}|\mathbf{X}) q^*(\mathbf{\theta} | \mathbf{X}) \Bigr) \\
=&\int_{\theta} \int_{\mathbf{Z}} q^*(\mathbf{Z}, \theta |\mathbf{X}) \log \frac{q^*(\mathbf{Z}, \theta |\mathbf{X})} 
{q^*(\mathbf{Z}|\mathbf{X}) q^*(\mathbf{\theta} | \mathbf{X})} \\
=& \mathcal{I} (\mathbf{Z}, \theta)
\end{align}
where $\mathcal{I} (\mathbf{Z}, \theta)$ is the mutual information between the variables $\mathbf{Z}$ and $\theta$.






\subsection{Mutual Info 2}

\subsubsection{Zero Mutual Info when decoder is not Bayesian}

First, we'll show that when $q(\theta)=\delta \theta$, $\mathcal{I}(\theta,z)=0$, meaning, if we are not being Bayesian about the weights of the decoder, there is no need to have $q(z|x)$ be dependent on $\theta$.
\begin{align}
    \mathcal{I} (\mathbf{Z}, \theta) &= \int_{\theta} \int_{\mathbf{Z}} q(\mathbf{Z}, \theta |\mathbf{X}) \log \frac{q(\mathbf{Z}, \theta |\mathbf{X})} 
    {q(\mathbf{Z}|\mathbf{X}) q(\mathbf{\theta} | \mathbf{X})} \\
    &= \int_{\theta} \int_{\mathbf{Z}} q(\mathbf{Z}| \theta,\mathbf{X})q(\theta|\mathbf{X}) \log \frac{q(\mathbf{Z}| \theta,\mathbf{X})q(\theta|\mathbf{X})} 
    {q(\mathbf{Z}|\mathbf{X}) q(\mathbf{\theta} | \mathbf{X})} \\
    &= \int_{\theta} \int_{\mathbf{Z}} q(\mathbf{Z}| \theta,\mathbf{X}) \delta \theta \log \frac{q(\mathbf{Z}| \theta,\mathbf{X})} 
    {q(\mathbf{Z}|\mathbf{X})} \\
    &= \int_{\mathbf{Z}} q(\mathbf{Z}|\mathbf{X}) \log \frac{q(\mathbf{Z}|\mathbf{X})} 
    {q(\mathbf{Z}|\mathbf{X})} \\
    &= 0
\end{align}
Instead of the above, I can just show that $q(z|\theta)=q(z)$ if $p(z,\theta)=p(z)p(\theta)$.
\begin{align}
    p(z,\theta) &= 
        \begin{cases*}
            p(z) & if $\theta$ = c \\
            0 & else
        \end{cases*} \\
    &= 
        \begin{cases*}
            p(z)p(\theta) & if $\theta$ = c \\
            p(z)p(\theta) & else
        \end{cases*} 
\end{align}
Thus, if $q(\theta)=\delta \theta$ then $\mathcal{I}(\theta,z)=0$. In contrast, if $H(\theta)>0$, then $\mathcal{I}(\theta,z) \geq 0$.


\subsubsection{Entropy of decoder is worse than MAP when q is factored}

\begin{align}
    E\left[ \frac{p(\theta_{MAP},z,x)}{q(z|x) \delta \theta} \right] & \geq E\left[ \frac{p(\theta_{MAP},z,x)}{q(z|x) q(\theta)} \right] 
\end{align}


\subsubsection{When its not factored, entropy can help}

$$\log p(x) = ELBO + KL$$
Thus we need to be generator-aware.





\section{Related Work}





\section{Experimental Results}


Objective maximized during training:
\begin{align}
 E_{q(\theta,z|x)} \left[\log \left( \frac{p(x,z)p(\theta)}{q(z|x,\theta)q(\theta)} \right) \right ] 
\end{align}
Objective evaluated on the test set:
\begin{align}
 E_{q(\theta)} \left[ E_{q(z'|x',\theta)} \left[\log \left( \frac{p(x',z'|\theta)}{q(z'|x',\theta)} \right) \right ] \right ] 
\end{align}

\subsection{Choice of Generator-Aware model}

\subsection{Generator-Aware with non-factorized inference and generator networks}

\begin{table}[h]
\centering
\label{table_ga}
\begin{tabular}{llcc}
                                           &            & \multicolumn{2}{c}{q(W)} \\ \cline{3-4} 
                                           &            & BNN         & MNF        \\
\multicolumn{1}{c|}{\multirow{2}{*}{q(z)}} & factorized & 167, 168    & 162, 194    \\
\multicolumn{1}{c|}{}                      & NF         & 166, 168     & 161, 185   
\end{tabular}
\caption{NLL on test set. Left is no GA, right is naive GA. }
\end{table}

\section{Conclusion}

We've introduced GABVAEs.


\bibliographystyle{apalike}
\bibliography{refs}


\newpage 

\section{Appendix}

Objective maximized during training:
\begin{align}
    p(x) &= \int_{\theta} \int_{z} p(x,z,\theta) \\
    &= E_{q(\theta,z|x)} \left[ \frac{p(x,z,\theta)}{q(\theta,z|x)} \right ] \\
    &= E_{q(\theta,z|x)} \left[ \frac{p(x,z)p(\theta)}{q(z|x,\theta)q(\theta)} \right ] \\
   \log(p(x)) &=\log \left( E_{q(\theta,z|x)} \left[ \frac{p(x,z)p(\theta)}{q(z|x,\theta)q(\theta)} \right ] \right) \\
    &\geq  E_{q(\theta,z|x)} \left[\log \left( \frac{p(x,z)p(\theta)}{q(z|x,\theta)q(\theta)} \right) \right ] 
\end{align}


Objective evaluated on the test set:
\begin{align}
    p(x'|x) &= \int_{\theta} \int_{z'} p(x',z',\theta|x) \\
    &= \int_{\theta} \int_{z} p(x',z'|\theta,x) p(\theta|x) \\
    &= \int_{\theta} \int_{z} p(x',z'|\theta) p(\theta|x) \\
    q(x'|x) &= \int_{\theta} \int_{z} p(x',z'|\theta) q(\theta) \\
    &= E_{q(\theta)} \left[ \int_{z} p(x',z'|\theta) \right] \\
    &= E_{q(\theta)} \left[ E_{q(z'|x')} \left[ \frac{p(x',z'|\theta)}{q(z'|x',\theta)} \right] \right]  \\
	log(q(x'|x)) &=\log \left(  E_{q(\theta)} \left[ E_{q(z'|x')} \left[ \frac{p(x',z'|\theta)}{q(z'|x',\theta)}  \right ] \right ] \right) \\
    &\geq   E_{q(\theta)} \left[ E_{q(z'|x')} \left[\log \left( \frac{p(x',z'|\theta)}{q(z'|x',\theta)} \right) \right ] \right ] 
\end{align}




% \footnotesize
% \begin{align}
% \hspace{-4.6cm} 0 &=? KL(p(z)p(\theta)\| p(z,\theta)) 
%     - MI(z,\theta) \nonumber \\
% \hspace{-4.6cm} &= p(z)p(\theta)\log p(z) 
%     + p(z)p(\theta) \log p(\theta)
%     - p(z)p(\theta)\log p(z|\theta)p(\theta) 
%     - p(z|\theta)p(\theta)\log p(z|\theta)p(\theta) 
%     + p(z|\theta)p(\theta)\log p(z)
%     + p(z|\theta)p(\theta)\log p(\theta) \nonumber \\
% \hspace{-4.6cm} &= p(z)p(\theta)\log p(z) 
%     + p(z)p(\theta)\log p(\theta)
%     - p(z)p(\theta)\log p(z|\theta)
%     - p(z)p(\theta)\log p(\theta)
%     - p(z|\theta)p(\theta)\log p(z|\theta) 
%     - p(z|\theta)p(\theta)\log p(\theta)
%     + p(z|\theta)p(\theta)\log p(z)
%     + p(z|\theta)p(\theta)\log p(\theta) \nonumber \\
% \hspace{-4.6cm} &= p(\theta) \cdot \left( p(z)\log p(z)
%     + p(z)\log p(\theta)
%     - p(z)\log p(z|\theta)
%     - p(z)\log p(\theta)
%     - p(z|\theta)\log p(z|\theta) 
%     - p(z|\theta)\log p(\theta)
%     + p(z|\theta)\log p(z)
%     + p(z|\theta)\log p(\theta) \right) \nonumber \\ 
% \hspace{-4.6cm} &= p(\theta) \cdot \left( p(z)\log p(z)
%     + p(z)\log p(\theta)
%     - p(z)\log p(z|\theta)
%     - p(z)\log p(\theta)
%     - p(z|\theta)\log p(z|\theta) 
%     + p(z|\theta)\log p(z)
%      \right) \nonumber  \\
% \hspace{-4.6cm} &= p(\theta) \cdot \left( p(z)\log p(z)
%     - p(z)\log p(z|\theta)
%     - p(z|\theta)\log p(z|\theta) 
%     + p(z|\theta)\log p(z)
%      \right) \nonumber  
% \end{align}



\end{document}
